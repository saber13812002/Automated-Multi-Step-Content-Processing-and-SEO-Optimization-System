import re
from pathlib import Path
from typing import List, Tuple
from rapidfuzz import fuzz, process
import datetime

def parse_srt(srt_path: str) -> List[dict]:
    """Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ SRT"""
    with open(srt_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ø§Ù„Ú¯ÙˆÛŒ regex Ø¨Ø±Ø§ÛŒ SRT
    pattern = r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.*?)(?=\n\n|\Z)'
    matches = re.findall(pattern, content, re.DOTALL)
    
    segments = []
    for match in matches:
        segments.append({
            'index': int(match[0]),
            'start': match[1],
            'end': match[2],
            'text': match[3].strip()
        })
    
    return segments

def normalize_text(text: str) -> str:
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ù‡ØªØ±"""
    # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
    text = re.sub(r'[^\w\s]', '', text)
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def align_srt_with_reference(
    srt_path: str,
    reference_txt_path: str,
    output_srt_path: str,
    threshold: int = 70
):
    """ØªØ·Ø¨ÛŒÙ‚ Ùˆ ØªØµØ­ÛŒØ­ SRT Ø¨Ø§ Ù…ØªÙ† Ù…Ø±Ø¬Ø¹
    
    Args:
        srt_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ SRT Ø§ÙˆÙ„ÛŒÙ‡ (Ø®Ø±ÙˆØ¬ÛŒ Whisper)
        reference_txt_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù…ØªÙ† Ù…Ø±Ø¬Ø¹ (Ø§Ø² Word)
        output_srt_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ SRT ØªØµØ­ÛŒØ­ Ø´Ø¯Ù‡
        threshold: Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø±ØµØ¯ ØªØ´Ø§Ø¨Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ·Ø¨ÛŒÙ‚ (0-100)
    """
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† SRT Ø§ÙˆÙ„ÛŒÙ‡
    segments = parse_srt(srt_path)
    print(f"ğŸ“„ ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ SRT: {len(segments)}")
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…ØªÙ† Ù…Ø±Ø¬Ø¹
    with open(reference_txt_path, 'r', encoding='utf-8') as f:
        reference_text = f.read()
    
    # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ù…Ø±Ø¬Ø¹ Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
    reference_sentences = re.split(r'[.!ØŸÛ”]\s+', reference_text)
    reference_sentences = [s.strip() for s in reference_sentences if s.strip()]
    
    print(f"ğŸ“š ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù…Ù„Ø§Øª Ù…Ø±Ø¬Ø¹: {len(reference_sentences)}")
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ
    normalized_refs = [normalize_text(s) for s in reference_sentences]
    
    # ØªØµØ­ÛŒØ­ Ù‡Ø± Ø¨Ø®Ø´
    corrected_segments = []
    matched_count = 0
    
    for i, seg in enumerate(segments):
        whisper_text = seg['text']
        whisper_normalized = normalize_text(whisper_text)
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ·Ø¨ÛŒÙ‚
        result = process.extractOne(
            whisper_normalized,
            normalized_refs,
            scorer=fuzz.ratio
        )
        
        if result and result[1] >= threshold:
            # ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØª Ø´Ø¯
            best_match_idx = result[2]
            corrected_text = reference_sentences[best_match_idx]
            matched_count += 1
            
            corrected_segments.append({
                'index': seg['index'],
                'start': seg['start'],
                'end': seg['end'],
                'text': corrected_text,
                'original': whisper_text,
                'similarity': result[1]
            })
        else:
            # ØªØ·Ø¨ÛŒÙ‚ Ù†ÛŒØ§ÙØª - Ø§Ø² Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            corrected_segments.append({
                'index': seg['index'],
                'start': seg['start'],
                'end': seg['end'],
                'text': whisper_text,
                'original': whisper_text,
                'similarity': 0
            })
    
    # Ø°Ø®ÛŒØ±Ù‡ SRT ØªØµØ­ÛŒØ­ Ø´Ø¯Ù‡
    with open(output_srt_path, 'w', encoding='utf-8') as f:
        for seg in corrected_segments:
            f.write(f"{seg['index']}\n")
            f.write(f"{seg['start']} --> {seg['end']}\n")
            f.write(f"{seg['text']}\n\n")
    
    print(f"âœ… SRT ØªØµØ­ÛŒØ­ Ø´Ø¯Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_srt_path}")
    print(f"ğŸ“Š ØªØ·Ø¨ÛŒÙ‚ ÛŒØ§ÙØªÙ‡: {matched_count}/{len(segments)} ({matched_count/len(segments)*100:.1f}%)")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ
    report_path = output_srt_path.replace('.srt', '_report.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("Ú¯Ø²Ø§Ø±Ø´ ØªØµØ­ÛŒØ­ SRT\n")
        f.write("=" * 80 + "\n\n")
        
        for seg in corrected_segments:
            if seg['text'] != seg['original']:
                f.write(f"â±ï¸  Ø²Ù…Ø§Ù†: {seg['start']} --> {seg['end']}\n")
                f.write(f"ğŸ”´ Ù‚Ø¨Ù„: {seg['original']}\n")
                f.write(f"ğŸŸ¢ Ø¨Ø¹Ø¯: {seg['text']}\n")
                f.write(f"ğŸ“Š Ø´Ø¨Ø§Ù‡Øª: {seg['similarity']:.1f}%\n")
                f.write("-" * 80 + "\n\n")
    
    print(f"ğŸ“‹ Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ: {report_path}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ØªØµØ­ÛŒØ­ SRT Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªÙ† Ù…Ø±Ø¬Ø¹')
    parser.add_argument('--srt', required=True, help='ÙØ§ÛŒÙ„ SRT Ø§ÙˆÙ„ÛŒÙ‡')
    parser.add_argument('--reference', required=True, help='ÙØ§ÛŒÙ„ Ù…ØªÙ† Ù…Ø±Ø¬Ø¹ (TXT)')
    parser.add_argument('--output', required=True, help='ÙØ§ÛŒÙ„ SRT Ø®Ø±ÙˆØ¬ÛŒ')
    parser.add_argument('--threshold', type=int, default=70, help='Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø±ØµØ¯ ØªØ´Ø§Ø¨Ù‡')
    
    args = parser.parse_args()
    
    align_srt_with_reference(
        args.srt,
        args.reference,
        args.output,
        args.threshold
    )