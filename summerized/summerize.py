from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

def summarize(text, model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)
    summary = summarizer(text, max_length=200, min_length=30, do_sample=False)

    return summary[0]['summary_text']


# # Ù…Ø«Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
# english_text = """
# Facebook is an American online social media and social networking service owned by Meta Platforms. 
# Founded in 2004 by Mark Zuckerberg with fellow Harvard College students, its name comes from the face book directories often given to American university students. 
# Membership was initially limited to Harvard students, gradually expanding to other North American universities and, since 2006, to anyone over 13 years old.
# """
# print("ğŸ”¹ English Summary:\n", summarize(english_text, "facebook/bart-large-cnn"))

# # Ù…Ø«Ø§Ù„ ÙØ§Ø±Ø³ÛŒ
# persian_text = """
# Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Ø§Ø³ÛŒ Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û³ÛµÛ¸ Ø¨Ù‡ ØªØµÙˆÛŒØ¨ Ø±Ø³ÛŒØ¯ Ùˆ Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û¶Û¸ Ù…ÙˆØ±Ø¯ Ø¨Ø§Ø²Ù†Ú¯Ø±ÛŒ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª. 
# Ø§ÛŒÙ† Ù‚Ø§Ù†ÙˆÙ† Ø§ØµÙˆÙ„ Ùˆ Ù…Ø¨Ø§Ù†ÛŒ Ø­Ú©ÙˆÙ…Øª Ø±Ø§ Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ù‚ÙˆØ§ØŒ Ø­Ù‚ÙˆÙ‚ Ù…Ù„Øª Ùˆ ÙˆØ¸Ø§ÛŒÙ Ø¯ÙˆÙ„Øª Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯. 
# Ø¨Ø§ Ú¯Ø°Ø´Øª Ø¨ÛŒØ´ Ø§Ø² Ú†Ù‡Ø§Ø± Ø¯Ù‡Ù‡ Ø§Ø² ØªØµÙˆÛŒØ¨ Ø¢Ù†ØŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙØ³ÛŒØ± Ùˆ Ø¨Ø§Ø²Ù†Ú¯Ø±ÛŒ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ø§ØµÙˆÙ„ Ø¢Ù† Ø§Ø­Ø³Ø§Ø³ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
# """
# print("\nğŸ”¹ Ø®Ù„Ø§ØµÙ‡ ÙØ§Ø±Ø³ÛŒ:\n", summarize(persian_text, "m3hrdadfi/bart-base-parsinlu-summary"))
